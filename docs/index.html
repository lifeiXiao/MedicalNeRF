<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="By combining 3D diffusion and NeRF representation into a holistic model, SSDNeRF learns powerful 3D generative prior from multi-view images, which can be exploited for unconditional generation and image-based 3D reconstruction.">
  <meta property="og:title" content="Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction"/>
  <meta property="og:description" content="By combining 3D diffusion and NeRF representation into a holistic model, SSDNeRF learns powerful 3D generative prior from multi-view images, which can be exploited for unconditional generation and image-based 3D reconstruction."/>
  <meta property="og:url" content="https://lakonik.github.io/ssdnerf"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <!-- <meta property="og:image" content="static/image/your_banner_image.png" /> -->
  <!-- <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->


  <meta name="twitter:title" content="Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction">
  <meta name="twitter:description" content="By combining 3D diffusion and NeRF representation into a holistic model, SSDNeRF learns powerful 3D generative prior from multi-view images, which can be exploited for unconditional generation and image-based 3D reconstruction.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="SSDNeRF, Diffusion, NeRF, Generation, Reconstruction, 3D">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction</h1>
          <h1 class="title is-3"><em>ICCV 2023</em></h1>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="https://lakonik.github.io/" target="_blank">Hansheng Chen</a><sup>1,</sup>*,
            </span>
            <span class="author-block">
              <a href="https://jiataogu.me/" target="_blank">Jiatao Gu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://apchenstu.github.io/" target="_blank">Anpei Chen</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=aYKQn88AAAAJ&hl=en" target="_blank">Wei Tian</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://pages.ucsd.edu/~ztu/" target="_blank">Zhuowen Tu</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://lingjie0206.github.io/" target="_blank">Lingjie Liu</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://cseweb.ucsd.edu/~haosu/" target="_blank">Hao Su</a><sup>4</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Tongji University &nbsp <sup>2</sup>Apple &nbsp <sup>3</sup>ETH Zürich &nbsp <sup>4</sup>UCSD &nbsp <sup>5</sup>University of Pennsylvania</span>
            <span class="eql-cntrb"><small><br>*Work done during a remote internship with UCSD</small></span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
                  <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2304.06714.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <!-- <span class="link-block">
                <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span> -->

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/Lakonik/SSDNeRF" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2304.06714" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="ai ai-arxiv"></i>
                </span>
                <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/teaser.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered is-5">
        <br><strong>TLDR:</strong> Trains 3D diffusion and NeRF jointly in a single stage<br> Comparable to/better than SoTAs on 3D generation and reconstruction
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D-aware image synthesis encompasses a variety of tasks, such as scene generation and novel view synthesis from images. Despite numerous task-specific methods, developing a comprehensive model remains challenging. In this paper, we present SSDNeRF, a unified approach that employs an expressive diffusion model to learn a generalizable prior of neural radiance fields (NeRF) from multi-view images of diverse objects. Previous studies have used two-stage approaches that rely on pretrained NeRFs as real data to train diffusion models. In contrast, we propose a new single-stage training paradigm with an end-to-end objective that jointly optimizes a NeRF auto-decoder and a latent diffusion model, enabling simultaneous 3D reconstruction and prior learning, even from sparsely available views. At test time, we can directly sample the diffusion prior for unconditional generation, or combine it with arbitrary observations of unseen objects for NeRF reconstruction. SSDNeRF demonstrates robust results comparable to or better than leading task-specific methods in unconditional generation and single/sparse-view 3D reconstruction.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body" style="padding-bottom:0rem">
    <div class="container is-max-desktop" >
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/teaser.png" alt="Failed to load image"/>
          <h2 class="subtitle has-text-justified is-6">
            <br>During training, SSDNeRF jointly learns triplane features of individual scenes, a shared NeRF decoder, and a triplane diffusion prior. During testing, it can perform (a) unconditional generation, (b) single-view reconstruction, as well as multi-view reconstruction.
          </h2>
        </div>
        <div class="item">
          <!-- Your image here -->
          <img src="static/images/framework.png" alt="Failed to load image"/>
          <h2 class="subtitle has-text-justified is-6">
            <br>An overview of SSDNeRF framework with a triplane NeRF representation. During training, we feed a batch of observations in the format of pixel RGBs and rays. The corresponding scene code is randomly initialized and optimized by minimizing both the rendering loss and the diffusion loss, and model parameters ϕ, ψ are also updated along the way.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<section class="section hero is-small" style="padding-bottom:0rem">
  <div class="hero-body" style="padding-bottom:0rem">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Unconditional Generation</h2>
      <div class="is-centered has-text-centered" style="padding-bottom:2rem">
        <video poster="" autoplay controls muted loop width="75%">
          <source src="static/videos/uncond_tables.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered is-6">
          Trained on ABO Tables
        </h2>
      </div>
      <div class="is-centered has-text-centered">
        <video poster="" autoplay controls muted loop width="75%">
          <source src="static/videos/uncond_cars.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered is-6">
          Trained on ShapeNet-SRN Cars
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small" style="padding-bottom:0rem">
  <div class="hero-body" style="padding-bottom:0rem">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Single-View Reconstruction</h2>
      <div class="is-centered has-text-centered" style="padding-bottom:2rem">
        <video poster="" autoplay controls muted loop width="70%">
          <source src="static/videos/singleview_cars.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered is-6">
          Inputs from ShapeNet-SRN Cars test set
        </h2>
      </div>
      <div class="is-centered has-text-centered" style="padding-bottom:2rem">
        <video poster="" autoplay controls muted loop width="70%">
          <source src="static/videos/singleview_chairs.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered is-6">
          Inputs from ShapeNet-SRN Chairs test set
        </h2>
      </div>
      <div class="is-centered has-text-centered">
        <video poster="" autoplay controls muted loop width="70%">
          <source src="static/videos/singleview_kitti.mp4"
          type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered is-6">
          [Sim-to-real] Inputs from KITTI real images (trained on SRN Cars)
        </h2>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-small" style="padding-bottom:0rem">
  <div class="hero-body" style="padding-bottom:0rem">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Sparse-to-Dense Reconstruction</h2>
      <div class="is-centered has-text-centered" style="padding-bottom:2rem">
        <img src="static/images/sparse2dense.png" alt="Failed to load image" width="50%" style="padding-top:0.5rem"/>
        <h2 class="subtitle is-6 has-text-centered">
          <br>Novel view synthesis quality (LPIPS) vs. number of input views, evaluated on SRN Cars.
        </h2>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{ssdnerf,
    title={Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction}, 
    author={Hansheng Chen and Jiatao Gu and Anpei Chen and Wei Tian and Zhuowen Tu and Lingjie Liu and Hao Su},
    year={2023},
    booktitle={ICCV}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

</body>
</html>
